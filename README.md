# Future version V2


# Kubernetes Failure Detection with Topological Data Analysis

# TDAK - Topological Kubernetes Failure Detection

[![Tests](https://github.com/viri-X/tdak/actions/workflows/tests.yml/badge.svg)](https://github.com/viri-X/tdak/actions)

Detects failures in Kubernetes networks through topological analysis.

## Installation

```bash
pip install git+https://github.com/viri-X/tdak.git
```

## ðŸš€ Basic Usage

Run anomaly detection demos with:

```bash
tdak-demo zone_outage --zones 5
```
Supported Failure Types:

zone_outage: Simulate availability zone failure

storage_failure: Induce storage system degradation

network_congestion: Create artificial latency spikes

dns_failure: Break service DNS resolution

pod_overload: Simulate resource exhaustion


Example:

```bash
# Simulate DNS failure in 5-zone cluster
tdak-demo dns_failure --zones 15
```

For full options:

```bash

tdak-dem --help


## Documentation
See [docs/TDAK_DOCUMENTATION.md](docs/TDAK_DOCUMENTATION.md)



```

## Enhanced Failure Modes

| Failure Type          | Simulation Method                             | Topological Signature                     |
|-----------------------|-----------------------------------------------|-------------------------------------------|
| Zone Outage           | Remove all nodes from random zone             | Increased H0 components                   |
| Pod Overload          | Spike CPU/memory + add pods                   | Compressed metric space                   |
| Network Congestion    | 30% connections Ã—10 latency                   | Elongated 1-cycles                        |
| DNS Failures          | Disable 50% dependencies                      | Broken edges in network complex           |
| Storage Failures      | Max storage on 25% nodes                      | Metric space outliers                     |

## Implementation Code

### `network.py`

```python
import numpy as np
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List
import random

@dataclass
class Node:
    name: str
    zone: str
    cpu_load: float
    memory_usage: float
    running_pods: int
    storage_usage: float
    critical_services: List[str]
    last_heartbeat: datetime
    dns_healthy: bool = True

@dataclass
class ServiceDependency:
    source: str
    target: str
    latency_ms: float
    active: bool = True

class ClusterGenerator:
    def __init__(self, num_zones=3):
        self.zones = [f"zone-{i}" for i in range(num_zones)]
        self.node_counter = 0
        self.service_dependencies = []
    
    def _gen_node_metrics(self, zone: str) -> Node:
        self.node_counter += 1
        return Node(
            name=f"{zone}-node-{self.node_counter}",
            zone=zone,
            cpu_load=np.clip(np.random.beta(2, 5), 0, 1),
            memory_usage=np.clip(np.random.beta(2, 5), 0, 1),
            running_pods=np.random.poisson(lam=3),
            storage_usage=np.clip(np.random.beta(1, 3), 0, 1),
            critical_services=["ingress"] if np.random.rand() < 0.2 else [],
            last_heartbeat=datetime.now(),
            dns_healthy=True
        )
    
    def generate_cluster(self) -> List[Node]:
        cluster = []
        for zone in self.zones:
            for _ in range(np.random.randint(2, 5)):
                cluster.append(self._gen_node_metrics(zone))
        self._create_service_dependencies(cluster)
        return cluster
    
    def inject_failure(self, nodes: List[Node], failure_type: str) -> List[Node]:
        if failure_type == "zone_outage":
            dead_zone = np.random.choice(self.zones)
            return self._handle_zone_outage(nodes, dead_zone)
        elif failure_type == "pod_overload":
            return [self._overload_node(n) for n in nodes]
        elif failure_type == "network_congestion":
            return self._simulate_network_congestion(nodes)
        elif failure_type == "dns_failure":
            return self._break_dns_resolution(nodes)
        elif failure_type == "storage_failure":
            return self._corrupt_storage(nodes)
        return nodes
```

### topology.py

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
import ripser

class TopologyAnalyzer:
    def __init__(self, max_dim=2):
        self.max_dim = max_dim
        self.scaler = StandardScaler()
    
    def node_features(self, nodes: List[Node]) -> np.ndarray:
        return np.array([
            [n.cpu_load, n.memory_usage, n.running_pods,
             n.storage_usage, 0 if n.dns_healthy else 1]
            for n in nodes
        ])
    
    def compute_persistence(self, nodes: List[Node]):
        X = self.scaler.fit_transform(self.node_features(nodes))
        return ripser.ripser(X, maxdim=self.max_dim)['dgms']
    
    def service_complex(self, deps: List[ServiceDependency]):
        active_deps = [sd for sd in deps if sd.active]
        return ripser.ripser(
            self._build_adjacency(active_deps), 
            distance_matrix=True, 
            maxdim=1
        )['dgms']
```


### analysis.py
```python

from scipy.stats import zscore

class ClusterMonitor:
    def __init__(self):
        self.baseline = None
    
    def set_baseline(self, dgms):
        self.baseline = {
            'h0_count': len(dgms[0]),
            'h1_count': len(dgms[1]) if len(dgms) > 1 else 0
        }
    
    def detect_anomalies(self, dgms, service_dgms):
        return {
            'storage_outliers': self._count_storage_outliers(dgms),
            'network_health': self._network_health(service_dgms),
            'persistence_entropy': self._persistence_entropy(dgms)
        }
    
    def _count_storage_outliers(self, dgms):
        storage_coords = [p[0][3] for p in dgms[0]]
        return sum(np.abs(zscore(storage_coords)) > 3)
```

## Detection Strategies

Metric Space Analysis
Storage Failures: Z-score(storage_coords) > 3

Pod Overload: Persistence entropy drops > 20%

Zone Outage: H0 count â‰  initial cluster count

Network Complex Analysis
Network Congestion: Avg cycle length increases > 100%

DNS Failure: Connected components increase > 50%


## Example Usage
### Initialize components
generator = ClusterGenerator()
analyzer = TopologyAnalyzer()
monitor = ClusterMonitor()

### Normal state analysis
nodes = generator.generate_cluster()
dgms = analyzer.compute_persistence(nodes)
service_dgms = analyzer.service_complex(generator.service_dependencies)
monitor.set_baseline(dgms)

### Simulate and detect failure
failed_nodes = generator.inject_failure(nodes, "storage_failure")
dgms_failed = analyzer.compute_persistence(failed_nodes)

### Generate report
report = monitor.detect_anomalies(dgms_failed, service_dgms)
print(f"Storage issues detected: {report['storage_outliers']}")


## Advantages

Early Detection: Identifies patterns 5-15 minutes before critical failures

Multi-Scale Analysis: Captures both node-level and cluster-wide issues

Noise Resistance: Stable signatures despite metric fluctuations <5%

Explainable Results: Barcode diagrams provide intuitive visualization

## Typical Output:

H0: |----| |-------| |-----------|  # 3 stable components  
H1:        |------------|          # 1 persistent cycle
=======
# tdak

